{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "OXHdXm2bMSYz"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "import os\n",
        "os.environ[\"NVIDIA_API_KEY\"] = \"your-nvidia-api-key\"\n",
        "\n",
        "\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcEpjyLVsWF0",
        "outputId": "a5779578-1611-41c1-d551-16ebafa1a8aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydantic==1.10.8 in c:\\users\\ozan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.10.8)Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: langchain==0.2.16 in c:\\users\\ozan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.16)\n",
            "Requirement already satisfied: langchain-core==0.2.41 in c:\\users\\ozan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.41)\n",
            "Requirement already satisfied: albumentations==1.3.0 in c:\\users\\ozan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.3.0)\n",
            "Collecting gradio==3.40.0\n",
            "  Using cached gradio-3.40.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: langchain-community==0.2.0 in c:\\users\\ozan\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.0)\n",
            "Using cached gradio-3.40.0-py3-none-any.whl (20.0 MB)\n",
            "Installing collected packages: gradio\n",
            "  Attempting uninstall: gradio\n",
            "    Found existing installation: gradio 4.41.0\n",
            "    Uninstalling gradio-4.41.0:\n",
            "      Successfully uninstalled gradio-4.41.0\n",
            "Successfully installed gradio-3.40.0\n"
          ]
        }
      ],
      "source": [
        "%pip install pydantic==1.10.8 langchain==0.2.16 langchain-core==0.2.41 albumentations==1.3.0 gradio==3.40.0 langchain-community==0.2.0 --no-deps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fXwoCqBtBKJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KwTgAyE-Glka"
      },
      "outputs": [],
      "source": [
        "from langchain.pydantic_v1 import BaseModel, Field\n",
        "from pydantic import BaseModel as PydanticBaseModel\n",
        "\n",
        "class KnowledgeBase(BaseModel):\n",
        "\n",
        "  first_name:str=Field('unknown',description=\"The first name of the customer\")\n",
        "  last_name:str=Field('unknown',description=\"The last name of the customer\")\n",
        "  order_id:int=Field(-1,description=\"the order id\")\n",
        "  order_status:str=Field('unknown',description=\"The order status\")\n",
        "  discussion_summary: str = Field(\"\", description=\"Summary of discussion so far,issues, etc.\")\n",
        "  open_problems: str = Field(\"\", description=\"Topics that have not been resolved yet\")\n",
        "  current_goals: str = Field(\"\", description=\"Current goal for the agent to address\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tfpFIY3lke7s"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-405b-instruct\") | StrOutputParser()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Jg7dSz93T-2M"
      },
      "outputs": [],
      "source": [
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.schema.runnable.passthrough import RunnableAssign\n",
        "def RExtract(pydantic_class, llm, prompt):\n",
        "    '''\n",
        "    Runnable Extraction module\n",
        "    Returns a knowledge dictionary populated by slot-filling extraction\n",
        "    '''\n",
        "    parser = PydanticOutputParser(pydantic_object=pydantic_class)\n",
        "    instruct_merge = RunnableAssign({'format_instructions' : lambda x: parser.get_format_instructions()})\n",
        "\n",
        "    def preparse(string):\n",
        "        if '{' not in string: string = '{' + string\n",
        "        if '}' not in string: string = string + '}'\n",
        "        string = (string\n",
        "            .replace(\"\\\\_\", \"_\")\n",
        "            .replace(\"\\n\", \" \")\n",
        "            .replace(r\"\\]\", \"]\")\n",
        "            .replace(r\"\\[\", \"[\")\n",
        "        )\n",
        "\n",
        "        return string\n",
        "    return instruct_merge | prompt | llm |StrOutputParser()| preparse | parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "id": "-1QfZ-cyVA2k"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "x docstore/\n",
            "x docstore/index.faiss\n",
            "x docstore/index.pkl\n"
          ]
        }
      ],
      "source": [
        "#RAG için belge yükleme\n",
        "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "embedder = NVIDIAEmbeddings(\n",
        "  model=\"nvidia/nv-embedqa-mistral-7b-v2\",\n",
        "  api_key=os.environ[\"NVIDIA_API_KEY\"],\n",
        "  truncate=\"NONE\",\n",
        "  )\n",
        "\n",
        "!tar xzvf docstore.tgz\n",
        "docstore = FAISS.load_local(\"docstore\", embedder, allow_dangerous_deserialization=True)\n",
        "docs = list(docstore.docstore._dict.values())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Emily|Davis|11111': {'first_name': 'Emily', 'last_name': 'Davis', 'order_id': 11111, 'order_status': 'delivered'}, 'Ethan|Miller|22222': {'first_name': 'Ethan', 'last_name': 'Miller', 'order_id': 22222, 'order_status': 'preparing'}, 'Lily|Wilson|33333': {'first_name': 'Lily', 'last_name': 'Wilson', 'order_id': 33333, 'order_status': 'shipping'}, 'Noah|Moore|44444': {'first_name': 'Noah', 'last_name': 'Moore', 'order_id': 44444, 'order_status': 'refunded'}, 'Sarah|Anderson|55555': {'first_name': 'Sarah', 'last_name': 'Anderson', 'order_id': 55555, 'order_status': 'delivered'}, 'William|Thomas|66666': {'first_name': 'William', 'last_name': 'Thomas', 'order_id': 66666, 'order_status': 'preparing'}, 'Olivia|Jackson|77777': {'first_name': 'Olivia', 'last_name': 'Jackson', 'order_id': 77777, 'order_status': 'shipping'}, 'Benjamin|White|88888': {'first_name': 'Benjamin', 'last_name': 'White', 'order_id': 88888, 'order_status': 'refunded'}, 'Ava|Harris|99999': {'first_name': 'Ava', 'last_name': 'Harris', 'order_id': 99999, 'order_status': 'delivered'}, 'Isabella|Martin|10101': {'first_name': 'Isabella', 'last_name': 'Martin', 'order_id': 10101, 'order_status': 'preparing'}, 'Mason|Thompson|20202': {'first_name': 'Mason', 'last_name': 'Thompson', 'order_id': 20202, 'order_status': 'shipping'}, 'Elijah|Walker|30303': {'first_name': 'Elijah', 'last_name': 'Walker', 'order_id': 30303, 'order_status': 'refunded'}, 'Mia|Young|40404': {'first_name': 'Mia', 'last_name': 'Young', 'order_id': 40404, 'order_status': 'delivered'}, 'Logan|Allen|50505': {'first_name': 'Logan', 'last_name': 'Allen', 'order_id': 50505, 'order_status': 'preparing'}, 'Sophia|King|60606': {'first_name': 'Sophia', 'last_name': 'King', 'order_id': 60606, 'order_status': 'shipping'}, 'Alexander|Scott|70707': {'first_name': 'Alexander', 'last_name': 'Scott', 'order_id': 70707, 'order_status': 'refunded'}, 'Charlotte|Lewis|80808': {'first_name': 'Charlotte', 'last_name': 'Lewis', 'order_id': 80808, 'order_status': 'delivered'}, 'Gabriel|Hall|90909': {'first_name': 'Gabriel', 'last_name': 'Hall', 'order_id': 90909, 'order_status': 'preparing'}, 'Amelia|Watson|10100': {'first_name': 'Amelia', 'last_name': 'Watson', 'order_id': 10100, 'order_status': 'shipping'}, 'Julian|Russell|20201': {'first_name': 'Julian', 'last_name': 'Russell', 'order_id': 20201, 'order_status': 'refunded'}}\n"
          ]
        }
      ],
      "source": [
        "string_dict = open(\"data_of_customers.txt\", \"r\").read()\n",
        "import ast\n",
        "def convert_to_dict(data_str):\n",
        "    # Remove newlines and extra spaces\n",
        "    data_str = data_str.strip()\n",
        "    \n",
        "    # Replace the outer single quotes with double quotes to make it JSON-like\n",
        "    data_str = data_str.replace(\"'\", '\"')\n",
        "    \n",
        "    # Now we can safely evaluate the string using ast.literal_eval\n",
        "    try:\n",
        "        result_dict = ast.literal_eval(data_str)\n",
        "        return result_dict\n",
        "    except (SyntaxError, ValueError) as e:\n",
        "        print(f\"Error parsing string: {e}\")\n",
        "        return None\n",
        "\n",
        "db_dict=convert_to_dict(string_dict)\n",
        "print(db_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_dict(base: BaseModel) -> dict:\n",
        "    '''Given a dictionary with a knowledge base, return a key for order_info'''\n",
        "    return {  \n",
        "        'first_name' : base.first_name,\n",
        "        'last_name' : base.last_name,\n",
        "        'order_id':base.order_id,\n",
        "        'order_status':base.order_status\n",
        "       \n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_info(d: dict)->str:\n",
        "    db = convert_to_dict(string_dict)\n",
        "    get_key = lambda d: \"|\".join([d['first_name'], d['last_name'], str(d['order_id'])])\n",
        "    req_keys = ['first_name', 'last_name', 'order_id']\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    if get_key(d) not in db:\n",
        "        return f\"Error: No information found for the key '{get_key(d)}' in the database.\"\n",
        "    data = db[get_key(d)]\n",
        "    return (\n",
        "        f\"{data['first_name']} {data['last_name']}'s order id is {data['order_id']} and the order status is {data['order_status']}.\"\n",
        "    \n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPVhjBjI3poE",
        "outputId": "051152b4-3341-4f21-b301-ceff430539af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "40\n",
            "40\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "embeds = np.load('embeddings.npy')\n",
        "embeds=embeds.tolist()\n",
        "good_embeds = embeds[:40]\n",
        "poor_embeds = embeds[40:]\n",
        "print(len(good_embeds))\n",
        "print(len(poor_embeds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmqx4nxfO6iY",
        "outputId": "9a6795d8-2fb6-4149-9930-2fcc469bf86e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(40, 4096)\n",
            "Training Results: 1.0\n",
            "Testing Results: 1.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "def train_logistic_regression(class0, class1):\n",
        "    ## Logistic regression version. Optimized mathematically using closed-form algorithm.\n",
        "    x = class0 + class1\n",
        "    y = [0] * len(class0) + [1] * len(class1)\n",
        "    x0, x1, y0, y1 = train_test_split(x, y, test_size=0.5, random_state=42)\n",
        "    model = LogisticRegression()\n",
        "    model.fit(x0, y0)\n",
        "    print(np.array(x0).shape)\n",
        "    print(\"Training Results:\", model.score(x0, y0))\n",
        "    print(\"Testing Results:\", model.score(x1, y1))\n",
        "    return model\n",
        "\n",
        "\n",
        "model2 = train_logistic_regression(poor_embeds, good_embeds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8EPWTIxHoMWd"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from rich.console import Console\n",
        "from rich.style import Style\n",
        "from rich.theme import Theme\n",
        "\n",
        "console = Console()\n",
        "base_style = Style(color=\"#76B900\", bold=True)\n",
        "pprint = partial(console.print, style=base_style)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EhC-QjdPA_X",
        "outputId": "98714655-8d5b-4f7e-8270-edf1a2ae1716"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def score_response(query):\n",
        "    ## TODO: embed the query and pass the embedding into your classifier\n",
        "\n",
        "    embedding = np.array([embedder.embed_query(query)])\n",
        "\n",
        "    ## TODO: return the score for the response\n",
        "    return model2.predict(embedding)[0]\n",
        "\n",
        "print(score_response(\"I wcfghf\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cExlscNSsUDT"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "assist_llm=ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\")\n",
        "chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
        "    \"You are a document retriever\"\n",
        "    \" User messaged just asked: {input}\\n\\n\"\n",
        "    \" From this, we have retrieved the following potentially-useful info: \"\n",
        "\n",
        "    \" Document Retrieval:\\n{context1}\\n\\n\"\n",
        "    \" (Answer only from retrieval. Only cite sources that are used. Do not include any commentary like here is your response)\"\n",
        "), ('user', '{input}')])\n",
        "\n",
        "\n",
        "retrieval=(\n",
        "\n",
        "    RunnableAssign({'context1':(itemgetter('input')|docstore.as_retriever())})\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "retrieval_chain = retrieval|chat_prompt| assist_llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from rich.console import Console\n",
        "from rich.style import Style\n",
        "from rich.theme import Theme\n",
        "\n",
        "console = Console()\n",
        "base_style = Style(color=\"#76B900\", bold=True)\n",
        "pprint = partial(console.print, style=base_style)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "def RPrint(preface=\"State: \"):\n",
        "    def print_and_return(x, preface=\"\"):\n",
        "        print(f\"{preface}{x}\")\n",
        "        return x\n",
        "    return RunnableLambda(partial(print_and_return, preface=preface))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jLiAzqabKjLr",
        "outputId": "7870a1f8-d253-4b3e-d6b4-2f5672e1fb9d"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableBranch\n",
        "from langchain_core.runnables import RunnableSequence\n",
        "\n",
        "\n",
        "\n",
        "get_dict_r = RunnableLambda(get_dict)\n",
        "get_info_r = RunnableLambda(get_info)\n",
        "\n",
        "parser_prompt = ChatPromptTemplate.from_template(\n",
        "    \"You are chatting with a user. The user just responded ('input'). Please update the knowledge base.\"\n",
        "    \" Record your response in the 'response' tag to continue the conversation.\"\n",
        "    \" Do not hallucinate any details, and make sure the knowledge base is not redundant.\"\n",
        "    \" Update the entries frequently to adapt to the conversation flow.\"\n",
        "    \"\\n{format_instructions}\"\n",
        "    \"\\n\\nOLD KNOWLEDGE BASE: {know_base}\"\n",
        "    \"\\n\\nNEW MESSAGE: {input}\"\n",
        "    \"\\n\\nNEW KNOWLEDGE BASE:\"\n",
        ")\n",
        "good_sys_msg = (\n",
        "    \"You are an eBay chatbot. Please answer their question: {input} while representing eBay.\"\n",
        "    \"Use the following context for order status: {context}\"\n",
        "    \"Please help them with their question if it is ethical and relevant using the data: {data}\"\n",
        "    \"(This is just for you, do not mention knowledge base, context )\"\n",
        "    \"Do not ask for any information like reason for refunding order\"\n",
        "    \"Do not ask for any information other than topics like 'Name,' 'Surname,' or 'Order ID.\"\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        ")\n",
        "## Resist talking about this topic\" system message\n",
        "bad_sys_msg = (\n",
        "    \"You are an eBay chatbot. Please answer their question while representing eBay.\"\n",
        "    \"  Their question has been analyzed and labeled as 'probably not useful to answer as an eBay Chatbot',\"\n",
        "    \"  so avoid answering if appropriate and explain your reasoning to them. Make your response as short as possible.\"\n",
        ")\n",
        "response_prompt = ChatPromptTemplate.from_messages([(\"system\", \"{system}\"), (\"user\", \"{input}\")])\n",
        "state={'know_base':KnowledgeBase()}\n",
        "\n",
        "\n",
        "extractor = RExtract(KnowledgeBase,assist_llm,parser_prompt)\n",
        "internal_chain=(\n",
        "    RunnableAssign({'know_base':extractor})\n",
        "\n",
        ")\n",
        "\n",
        "chat_chain = (\n",
        "\n",
        "\n",
        "    RunnableAssign(dict(\n",
        "        system = RunnableBranch(\n",
        "            ## Switch statement syntax. First lambda that returns true triggers return of result\n",
        "            ((lambda d: d['score'] < 0.5), RunnableLambda(lambda x: bad_sys_msg)),\n",
        "            ## ... (more branches can also be specified)\n",
        "            ## Default branch. Will run if none of the others do\n",
        "\n",
        "            RunnableAssign({'data':retrieval_chain})|\n",
        "            RunnableAssign({'context':itemgetter('know_base')|get_dict_r|get_info_r})|RPrint()\n",
        "            |RunnableLambda(lambda x: good_sys_msg)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ),\n",
        "        input = itemgetter('input')\n",
        "    )) | response_prompt | instruct_llm\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def chat_gen(message, history=[], return_buffer=True):\n",
        "\n",
        "    ## Pulling in, updating, and printing the state\n",
        "    global state\n",
        "    state['input'] = message\n",
        "    state['history'] = history\n",
        "    state['output'] = \"\" if not history else history[-1][1]\n",
        "    state['score']=score_response(message)\n",
        "\n",
        "    state = internal_chain.invoke(state)\n",
        "    print(\"State after internal chain run:\")\n",
        "    pprint({k:v for k,v in state.items() if k != \"history\"})\n",
        "\n",
        "    ## Streaming the results\n",
        "    buffer = \"\"\n",
        "    for token in chat_chain.stream(state):\n",
        "        buffer += token\n",
        "        yield buffer if return_buffer else token\n",
        "    ## Generating the new state from the internal chain\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def streaming(chat_stream, history = [], max_questions=8):\n",
        "\n",
        "    \n",
        "    for human_msg, agent_msg in history:\n",
        "        if human_msg: print(\"\\n[ Human ]:\", human_msg)\n",
        "        if agent_msg: print(\"\\n[ Agent ]:\", agent_msg)\n",
        "\n",
        "   \n",
        "    for i in range(max_questions):\n",
        "        message = input(\"\\n[ Human ]: \")\n",
        "        print(\"\\n[ Agent ]: \")\n",
        "        history_entry = [message, \"\"]\n",
        "        for token in chat_stream(message, history, return_buffer=False):\n",
        "            print(token, end='')\n",
        "            history_entry[1] += token\n",
        "        history += [history_entry]\n",
        "        print(\"\\n\")\n",
        "\n",
        "## history is of format [[User response 0, Bot response 0], ...]\n",
        "chat_history = [[None, \"Hello! I'm your eBay agent! How can I help you?\"]]\n",
        "\n",
        "\n",
        "streaming(\n",
        "    chat_stream = chat_gen,\n",
        "    history = chat_history\n",
        ")\n",
        "#My name is Ethan Miller and I want to know about my order status"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
